{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "\n",
    "train_path = os.path.join(DATA_PATH, 'train')\n",
    "val_path = os.path.join(DATA_PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "IMAGE_SHAPE = 224\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAugTransform:\n",
    "    def __init__(self, rotation):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Scale((IMAGE_SHAPE, IMAGE_SHAPE)),\n",
    "            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Affine(rotate=rotation, mode='symmetric'),\n",
    "            iaa.Sometimes(0.2, iaa.Dropout(p=(0, 0.1))),\n",
    "            iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "        ])\n",
    "      \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = self.aug.augment_image(img)\n",
    "        img = np.transpose(img, [2, 0, 1]) / np.max(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Scale((IMAGE_SHAPE, IMAGE_SHAPE)),\n",
    "        ])\n",
    "      \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = self.aug.augment_image(img)\n",
    "        img = np.transpose(img, [2, 0, 1]) / np.max(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "human_dataset1 = ImageFolder(train_path, transform=TrainAugTransform(-12))\n",
    "human_dataset2 = ImageFolder(train_path, transform=TrainAugTransform(0))\n",
    "human_dataset3 = ImageFolder(train_path, transform=TrainAugTransform(12))\n",
    "human_dataset = ConcatDataset([human_dataset1, human_dataset2, human_dataset3])\n",
    "\n",
    "data_loader = DataLoader(human_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(ImageFolder(val_path, transform=TestAugTransform()), batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleVGGClassifier(nn.Module):\n",
    "    def __init__(self, image_shape):\n",
    "        super().__init__()\n",
    "        self.kernel_size = 3\n",
    "        self.conv1 = self.conv_block(3, 64, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = self.conv_block(64, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = self.conv_block(128, 256, 4)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = self.conv_block(256, 512, 4)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        # n x 512 x 14 x 14\n",
    "\n",
    "        self.linear1 = nn.Linear(512 * 14 * 14, 1000)\n",
    "        self.lrelu1 = nn.LeakyReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.linear2 = nn.Linear(1000, 1000)\n",
    "        self.lrelu2 = nn.LeakyReLU()\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.linear3 = nn.Linear(1000, 2)\n",
    "        \n",
    "    def conv_block(self, start_volume, end_volume, num_conv=1) -> nn.Module:\n",
    "        block = nn.Sequential()\n",
    "        for i in range(num_conv):\n",
    "            name_conv, name_relu = 'conv_{}_{}'.format(i, start_volume), 'relu_{}'.format(i)\n",
    "            block.add_module(name_conv, nn.Conv2d(start_volume if i == 0 else end_volume, \n",
    "                                                  end_volume, \n",
    "                                                  self.kernel_size,\n",
    "                                                  padding=1))\n",
    "            block.add_module(name_relu, nn.ReLU())\n",
    "        name_pool = 'pool_{}'.format(start_volume)\n",
    "        block.add_module(name_pool, nn.MaxPool2d(kernel_size=2))\n",
    "        return block\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.linear1(x.view(input.shape[0], -1))\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.linear3(x)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adagrad\n",
    "\n",
    "model = SimpleVGGClassifier(IMAGE_SHAPE).cuda() if use_cuda else SimpleVGGClassifier(IMAGE_SHAPE)\n",
    "opt = Adagrad(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [25:38<00:00, 21.07s/it]\n",
      "  0%|          | 0/9 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (12,2) into shape (12)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-59-fae737babb14>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m             \u001B[0mcuda_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcuda_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m             \u001B[0mval_model_result\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mBATCH_SIZE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mi\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mBATCH_SIZE\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary_cross_entropy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcuda_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m             \u001B[0mtmp_val_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: could not broadcast input array from shape (12,2) into shape (12)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    tmp_train_loss, tmp_val_loss = [], []\n",
    "\n",
    "    model = model.train()\n",
    "    for batch, labels in tqdm(data_loader, file=sys.stderr):\n",
    "        opt.zero_grad()\n",
    "        cuda_batch = batch.type(dtype)\n",
    "        cuda_labels = labels.type(dtype)\n",
    "        result = model(cuda_batch)\n",
    "        loss = F.binary_cross_entropy(result[:, 1], cuda_labels)\n",
    "        tmp_train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    val_model_result = np.zeros((len(val_loader) * BATCH_SIZE, 2))\n",
    "\n",
    "    model = model.eval()\n",
    "    for i, (batch, labels) in enumerate(tqdm(val_loader, file=sys.stderr)):\n",
    "        with torch.no_grad():\n",
    "            cuda_batch = batch.type(dtype)\n",
    "            cuda_labels = labels.type(dtype)\n",
    "            result = model(cuda_batch)\n",
    "            val_model_result[i * BATCH_SIZE:i * BATCH_SIZE + len(result)] = result.clone().detach().cpu()\n",
    "            loss = F.binary_cross_entropy(result[:, 1], cuda_labels)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "\n",
    "    train_loss.append(np.mean(tmp_train_loss))\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "\n",
    "    print(f'[INFO] Train loss: {train_loss[-1]}, Validation loss: {val_loss[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "val_model_result = np.zeros(len(val_loader) * BATCH_SIZE)\n",
    "real_targets = np.zeros(len(val_loader) * BATCH_SIZE)\n",
    "\n",
    "model = model.eval()\n",
    "for i, (batch, labels) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        cuda_batch = batch.type(dtype)\n",
    "        cuda_labels = labels.type(dtype)\n",
    "        result = model(cuda_batch)\n",
    "        print(labels, result)\n",
    "        _res = result.clone().detach().cpu().numpy().argmax(axis=1)\n",
    "        val_model_result[(i * BATCH_SIZE):(i * BATCH_SIZE + len(result))] = _res\n",
    "        real_targets[(i * BATCH_SIZE):(i * BATCH_SIZE + len(result))] = labels\n",
    "        loss = F.binary_cross_entropy(result[:, 1], cuda_labels)\n",
    "        tmp_val_loss.append(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_model_result, real_targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(real_targets, val_model_result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), 'model.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0]) tensor([[0.8032, 0.1968],\n",
      "        [0.6820, 0.3180],\n",
      "        [0.8772, 0.1228],\n",
      "        [0.4276, 0.5724],\n",
      "        [0.6961, 0.3039],\n",
      "        [0.5297, 0.4703],\n",
      "        [0.5848, 0.4152],\n",
      "        [0.9096, 0.0904],\n",
      "        [0.5692, 0.4308],\n",
      "        [0.4871, 0.5129],\n",
      "        [0.7938, 0.2062],\n",
      "        [0.6524, 0.3476]])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1]) tensor([[0.8155, 0.1845],\n",
      "        [0.5058, 0.4942],\n",
      "        [0.8879, 0.1121],\n",
      "        [0.8689, 0.1311],\n",
      "        [0.5196, 0.4804],\n",
      "        [0.7906, 0.2094],\n",
      "        [0.3823, 0.6177],\n",
      "        [0.8359, 0.1641],\n",
      "        [0.4563, 0.5437],\n",
      "        [0.8875, 0.1125],\n",
      "        [0.2658, 0.7342],\n",
      "        [0.4357, 0.5643]])\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]) tensor([[0.4539, 0.5461],\n",
      "        [0.3347, 0.6653],\n",
      "        [0.6576, 0.3424],\n",
      "        [0.3798, 0.6202],\n",
      "        [0.4152, 0.5848],\n",
      "        [0.2745, 0.7255],\n",
      "        [0.5442, 0.4558],\n",
      "        [0.6612, 0.3388],\n",
      "        [0.2548, 0.7452],\n",
      "        [0.3556, 0.6444],\n",
      "        [0.6386, 0.3614],\n",
      "        [0.6963, 0.3037]])\n",
      "tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0]) tensor([[0.5191, 0.4809],\n",
      "        [0.7127, 0.2873],\n",
      "        [0.8592, 0.1408],\n",
      "        [0.8709, 0.1291],\n",
      "        [0.6946, 0.3054],\n",
      "        [0.7223, 0.2777],\n",
      "        [0.6114, 0.3886],\n",
      "        [0.2183, 0.7817],\n",
      "        [0.5370, 0.4630],\n",
      "        [0.7137, 0.2863],\n",
      "        [0.3363, 0.6637],\n",
      "        [0.6334, 0.3666]])\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1]) tensor([[0.3495, 0.6505],\n",
      "        [0.5594, 0.4406],\n",
      "        [0.5907, 0.4093],\n",
      "        [0.6542, 0.3458],\n",
      "        [0.2085, 0.7915],\n",
      "        [0.9172, 0.0828],\n",
      "        [0.3334, 0.6666],\n",
      "        [0.8433, 0.1567],\n",
      "        [0.2040, 0.7960],\n",
      "        [0.7040, 0.2960],\n",
      "        [0.5868, 0.4132],\n",
      "        [0.7290, 0.2710]])\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]) tensor([[0.5278, 0.4722],\n",
      "        [0.4514, 0.5486],\n",
      "        [0.1757, 0.8243],\n",
      "        [0.4874, 0.5126],\n",
      "        [0.5831, 0.4169],\n",
      "        [0.8459, 0.1541],\n",
      "        [0.8506, 0.1494],\n",
      "        [0.1422, 0.8578],\n",
      "        [0.8069, 0.1931],\n",
      "        [0.8827, 0.1173],\n",
      "        [0.3081, 0.6919],\n",
      "        [0.4575, 0.5425]])\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1]) tensor([[0.3246, 0.6754],\n",
      "        [0.7745, 0.2255],\n",
      "        [0.5431, 0.4569],\n",
      "        [0.2214, 0.7786],\n",
      "        [0.4874, 0.5126],\n",
      "        [0.3496, 0.6504],\n",
      "        [0.6975, 0.3025],\n",
      "        [0.7226, 0.2774],\n",
      "        [0.7111, 0.2889],\n",
      "        [0.7490, 0.2510],\n",
      "        [0.5933, 0.4067],\n",
      "        [0.7423, 0.2577]])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]) tensor([[0.9166, 0.0834],\n",
      "        [0.3876, 0.6124],\n",
      "        [0.5248, 0.4752],\n",
      "        [0.6120, 0.3880],\n",
      "        [0.6344, 0.3656],\n",
      "        [0.8476, 0.1524],\n",
      "        [0.6405, 0.3595],\n",
      "        [0.8877, 0.1123],\n",
      "        [0.6730, 0.3270],\n",
      "        [0.5086, 0.4914],\n",
      "        [0.7276, 0.2724],\n",
      "        [0.8700, 0.1300]])\n",
      "tensor([0, 1, 0, 1]) tensor([[0.7582, 0.2418],\n",
      "        [0.8698, 0.1302],\n",
      "        [0.4359, 0.5641],\n",
      "        [0.7938, 0.2062]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]),\n",
       " array([1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_model_result, real_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.76      0.65        58\n",
      "         1.0       0.55      0.34      0.42        50\n",
      "\n",
      "    accuracy                           0.56       108\n",
      "   macro avg       0.56      0.55      0.54       108\n",
      "weighted avg       0.56      0.56      0.54       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(real_targets, val_model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), 'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}