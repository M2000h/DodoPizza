{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "\n",
    "train_path = os.path.join(DATA_PATH, 'train')\n",
    "val_path = os.path.join(DATA_PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "IMAGE_SHAPE = 224\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, torch.FloatTensor)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAugTransform:\n",
    "    def __init__(self, rotation):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Scale((IMAGE_SHAPE, IMAGE_SHAPE)),\n",
    "            iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Affine(rotate=rotation, mode='symmetric'),\n",
    "            iaa.Sometimes(0.2, iaa.Dropout(p=(0, 0.1))),\n",
    "            iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "        ])\n",
    "      \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = self.aug.augment_image(img)\n",
    "        img = np.transpose(img, [2, 0, 1]) / np.max(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "            iaa.Scale((IMAGE_SHAPE, IMAGE_SHAPE)),\n",
    "        ])\n",
    "      \n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = self.aug.augment_image(img)\n",
    "        img = np.transpose(img, [2, 0, 1]) / np.max(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\imgaug\\imgaug.py:184: DeprecationWarning: Function `Scale()` is deprecated. Use `Resize` instead. Resize has the exactly same interface as Scale.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "human_dataset1 = ImageFolder(train_path, transform=TrainAugTransform(-12))\n",
    "human_dataset2 = ImageFolder(train_path, transform=TrainAugTransform(0))\n",
    "human_dataset3 = ImageFolder(train_path, transform=TrainAugTransform(12))\n",
    "human_dataset = ConcatDataset([human_dataset1, human_dataset2, human_dataset3])\n",
    "\n",
    "data_loader = DataLoader(human_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(ImageFolder(val_path, transform=TestAugTransform()), batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleVGGClassifier(nn.Module):\n",
    "    def __init__(self, image_shape):\n",
    "        super().__init__()\n",
    "        self.kernel_size = 3\n",
    "        self.conv1 = self.conv_block(3, 64, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = self.conv_block(64, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = self.conv_block(128, 256, 4)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = self.conv_block(256, 512, 4)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        # n x 512 x 14 x 14\n",
    "\n",
    "        self.linear1 = nn.Linear(512 * 14 * 14, 1000)\n",
    "        self.lrelu1 = nn.LeakyReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.linear2 = nn.Linear(1000, 1000)\n",
    "        self.lrelu2 = nn.LeakyReLU()\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.linear3 = nn.Linear(1000, 2)\n",
    "        \n",
    "    def conv_block(self, start_volume, end_volume, num_conv=1) -> nn.Module:\n",
    "        block = nn.Sequential()\n",
    "        for i in range(num_conv):\n",
    "            name_conv, name_relu = 'conv_{}_{}'.format(i, start_volume), 'relu_{}'.format(i)\n",
    "            block.add_module(name_conv, nn.Conv2d(start_volume if i == 0 else end_volume, \n",
    "                                                  end_volume, \n",
    "                                                  self.kernel_size,\n",
    "                                                  padding=1))\n",
    "            block.add_module(name_relu, nn.ReLU())\n",
    "        name_pool = 'pool_{}'.format(start_volume)\n",
    "        block.add_module(name_pool, nn.MaxPool2d(kernel_size=2))\n",
    "        return block\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.linear1(x.view(input.shape[0], -1))\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.linear3(x)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adagrad\n",
    "\n",
    "model = SimpleVGGClassifier(IMAGE_SHAPE).cuda() if use_cuda else SimpleVGGClassifier(IMAGE_SHAPE)\n",
    "opt = Adagrad(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/397 [00:23<2:31:48, 23.00s/it]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    tmp_train_loss, tmp_val_loss = [], []\n",
    "\n",
    "    model = model.train()\n",
    "    for batch, labels in tqdm(data_loader, file=sys.stderr):\n",
    "        opt.zero_grad()\n",
    "        cuda_batch = batch.type(dtype)\n",
    "        cuda_labels = labels.type(dtype)\n",
    "        result = model(cuda_batch)\n",
    "        loss = F.binary_cross_entropy(result[:, 1], cuda_labels)\n",
    "        tmp_train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    val_model_result = np.zeros((len(val_loader) * BATCH_SIZE, 2))\n",
    "\n",
    "    model = model.eval()\n",
    "    for i, (batch, labels) in enumerate(tqdm(val_loader, file=sys.stderr)):\n",
    "        with torch.no_grad():\n",
    "            cuda_batch = batch.type(dtype)\n",
    "            cuda_labels = labels.type(dtype)\n",
    "            result = model(cuda_batch)\n",
    "            val_model_result[i * BATCH_SIZE:i * BATCH_SIZE + len(result)] = result.clone().detach().cpu()\n",
    "            loss = F.binary_cross_entropy(result[:, 1], cuda_labels)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "\n",
    "    train_loss.append(np.mean(tmp_train_loss))\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "\n",
    "    print(f'[INFO] Train loss: {train_loss[-1]}, Validation loss: {val_loss[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "val_model_result = np.zeros(len(val_loader) * BATCH_SIZE)\n",
    "real_targets = np.zeros(len(val_loader) * BATCH_SIZE)\n",
    "\n",
    "model = model.eval()\n",
    "for i, (batch, labels) in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        cuda_batch = batch.type(dtype)\n",
    "        cuda_labels = labels.type(dtype)\n",
    "        result = model(cuda_batch)\n",
    "        print(labels, result)\n",
    "        _res = result.clone().detach().cpu().numpy().argmax(axis=1)\n",
    "        val_model_result[(i * BATCH_SIZE):(i * BATCH_SIZE + len(result))] = _res\n",
    "        real_targets[(i * BATCH_SIZE):(i * BATCH_SIZE + len(result))] = labels\n",
    "        loss = F.binary_cross_entropy(result[:, 1], cuda_labels)\n",
    "        tmp_val_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_model_result, real_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(real_targets, val_model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), 'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model_result, real_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(real_targets, val_model_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(os.getcwd(), 'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}